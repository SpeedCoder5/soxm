{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab06a KMeans\n",
    "\n",
    "This lab explores the differences between `NearestNeighbors` and `KMeans` and discuss considerations when working with untagged data.\n",
    "\n",
    "### Nearest Neighbors (`NearestNeighbors`)\n",
    "\n",
    "**Import Statement:**\n",
    "```python\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "```\n",
    "\n",
    "**Purpose:**\n",
    "- `NearestNeighbors` is primarily used for finding the nearest neighbors of data points.\n",
    "- It is a versatile algorithm that can be used for both supervised and unsupervised learning scenarios.\n",
    "- In the context of clustering, though not typical, it can be adapted to measure distances and group data points based on proximity.\n",
    "\n",
    "**How it works:**\n",
    "- It computes the distance between data points and identifies the nearest neighbors based on a chosen distance metric (e.g., Euclidean, Manhattan).\n",
    "- For supervised learning, it is typically used in classification tasks (e.g., k-NN classifier).\n",
    "- For unsupervised learning, it might be used in anomaly detection or in an adapted manner to identify groupings.\n",
    "\n",
    "### K-Means (`KMeans`)\n",
    "\n",
    "**Import Statement:**\n",
    "```python\n",
    "from sklearn.cluster import KMeans\n",
    "```\n",
    "\n",
    "**Purpose:**\n",
    "- `KMeans` is specifically designed for clustering.\n",
    "- It is unsupervised and does not require prior labels or tags on the data.\n",
    "\n",
    "**How it works:**\n",
    "- The algorithm partitions the data into `k` clusters.\n",
    "- It initializes `k` centroids (cluster centers) randomly.\n",
    "- It then iteratively assigns data points to the nearest centroid and updates the centroids based on the mean of the assigned points.\n",
    "- This process continues until convergence (i.e., the centroids no longer change significantly).\n",
    "\n",
    "### When Data is Not Tagged with True Values\n",
    "\n",
    "When your data is not tagged with true values (which is a typical scenario in clustering), the goal is to uncover the natural structure in the data. Here are the key points to consider:\n",
    "\n",
    "1. **Clustering Algorithms like KMeans:**\n",
    "   - **Effective Use**: Algorithms like `KMeans` do not require true labels beforehand. They are designed to find patterns and group similar data points together based purely on the data's intrinsic properties.\n",
    "   - **Initialization**: Ensure careful initialization of centroids (e.g., using `k-means++` to improve convergence).\n",
    "\n",
    "2. **Evaluation Metrics:**\n",
    "   - **Internal Validation**: Use metrics like the silhouette score, inertia, or Davies-Bouldin index which do not require ground truth labels to evaluate the quality of clusters.\n",
    "   - **Visualization**: Plotting the clusters can also help in visually assessing the clustering quality.\n",
    "\n",
    "3. **Choosing the Number of Clusters:**\n",
    "   - **Elbow Method**: Plot the within-cluster sum of squares (also known as inertia) for different values of `k` (number of clusters) and look for an \"elbow\" point.\n",
    "   - **Silhouette Analysis**: Calculate the silhouette score for different values of `k` to determine the number of clusters that best fits the data.\n",
    "\n",
    "Hereâ€™s an example using `KMeans` for clustering untagged data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "\n",
    "# Apply KMeans clustering\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "df['cluster'] = kmeans.fit_predict(df[iris.feature_names])\n",
    "\n",
    "# Visualizing the clusters using first two features\n",
    "fig = px.scatter(df, x='sepal length (cm)', y='sepal width (cm)', color='cluster', \n",
    "                 title='KMeans Clustering (Sepal Length vs Sepal Width)')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elbow Method Example with KMeans\n",
    "\n",
    "Use this method to determine the number of clusters.\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "1. **Imports**:\n",
    "   - We import necessary libraries including pandas, plotly for plotting, and sklearn for KMeans.\n",
    "2. **Data Loading**:\n",
    "   - Load the Iris dataset and convert it to a pandas DataFrame.\n",
    "3. **Inertia Calculation**: \n",
    "   - Define a function `calculate_inertia` that computes the within-cluster sum of squares (inertia) for different values of \\( k \\) ranging from 1 to `max_k`.\n",
    "   - Loop over a range of \\( k \\) values, fit the KMeans model, and store the results.\n",
    "4. **Elbow Plot**:\n",
    "   - Create a scatter plot using Plotly, showing the relationship between the number of clusters \\( k \\) and the calculated inertia.\n",
    "   - Look for the \"elbow\" point on the plot to determine the optimal number of clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate inertia for different k values\n",
    "def calculate_inertia(data, max_k):\n",
    "    inertias = []\n",
    "    for k in range(1, max_k + 1):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        kmeans.fit(data)\n",
    "        inertias.append(kmeans.inertia_)\n",
    "    return inertias\n",
    "\n",
    "# Calculate inertia for k values from 1 to 10\n",
    "max_k = 10\n",
    "inertias = calculate_inertia(df[iris.feature_names], max_k)\n",
    "\n",
    "# Visualize the Elbow Method\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=list(range(1, max_k + 1)),\n",
    "    y=inertias,\n",
    "    mode='lines+markers',\n",
    "    marker=dict(color='blue'),\n",
    "    name=\"Inertia (Within-cluster Sum of Squares)\"\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Elbow Method for Determining Optimal Number of Clusters\",\n",
    "    xaxis_title=\"Number of Clusters (k)\",\n",
    "    yaxis_title=\"Inertia\",\n",
    "    showlegend=False\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this example, you will see that the optimal number of clusters for the Iris dataset is indeed \\( k = 3 \\), validating what we already know. However, this method can be applied to any dataset to determine the appropriate number of clusters.\n",
    "\n",
    "### Summary\n",
    "- **`NearestNeighbors`** is versatile and typically used for finding nearest neighbors, with applications in both classification and anomaly detection.\n",
    "- **`KMeans`** is specifically tailored for clustering tasks and does not require labeled data.\n",
    "- When data is not tagged with true values, use appropriate clustering algorithms, evaluation metrics, and visualization techniques to uncover and assess the natural groupings in your data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "soxm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
