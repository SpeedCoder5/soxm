{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab04: Tokens, Embeddings, and Distance\n",
    "\n",
    "Start by copying this lab notebook into your notebook folder, and run it step by step from there.\n",
    "\n",
    "What this lab covers:\n",
    "\n",
    "1. **Imports Libraries and Sets Up API Key**: Imports necessary libraries and sets up the OpenAI API key.\n",
    "2. **Helper Functions**: \n",
    "   - `get_embeddings(text)`: Fetches tokens and embeddings for the input text.\n",
    "   - `display_tokens_and_embeddings(text)`: Displays tokens, the number of tokens, and the first 5 dimensions of embeddings.\n",
    "3. **Example Texts for LeBron James**: Breaks down and displays tokens and embeddings for multiple text examples related to LeBron James.\n",
    "4. **Example Texts for Michael Jordan**: Does the same for Michael Jordan.\n",
    "5. **Distance Metrics**: Provides a brief explanation of different distance metrics.\n",
    "6. **Cosine Similarity Calculations**: \n",
    "   - Compares all LeBron James examples to each other.\n",
    "   - Compares all Michael Jordan examples to each other.\n",
    "   - Aggregates embeddings for LeBron James and Michael Jordan, then compares the aggregated vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from dotenv import dotenv_values\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "import re\n",
    "import tiktoken # see \n",
    "\n",
    "# Set up your OpenAI API key\n",
    "print(f'Using openai {openai.__version__}')\n",
    "config = dotenv_values()\n",
    "openai_api_key = config['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to get tokens and embeddings\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=openai_api_key,\n",
    ")\n",
    "\n",
    "# encoder choices are ['gpt2', 'r50k_base', 'p50k_base', 'p50k_edit', 'cl100k_base', 'o200k_base']\n",
    "encoder = tiktoken.get_encoding(\"o200k_base\")\n",
    "def get_tokens(text):\n",
    "    return encoder.encode(text)\n",
    "\n",
    "def get_embeddings(text):\n",
    "    response = client.embeddings.create(input=text, model=\"text-embedding-ada-002\")\n",
    "    return response.data[0].embedding\n",
    "\n",
    "def display_tokens_and_embeddings(text):\n",
    "    tokens = get_tokens(text)\n",
    "    embeddings = get_embeddings(text)\n",
    "    print(f\"\\n--- Example for: '{text}' ---\")\n",
    "    print(\"Text:\", text)\n",
    "    print(\"\\nTokens:\")\n",
    "    for token in tokens:\n",
    "        print(token, end=\" | \")\n",
    "\n",
    "    print(f\"\\n\\nNumber of Tokens: {len(tokens)}\")\n",
    "    print(\"\\nEmbeddings (first 5 dimensions for each token):\")\n",
    "    for i, token in enumerate(tokens):\n",
    "        print(f\"Token '{token}': {embeddings[i*5:(i+1)*5]}\")\n",
    "\n",
    "    print(f\"\\nEmbedding length: {len(embeddings)}\")\n",
    "    return np.array(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LeBron James Examples\n",
    "\n",
    "Understanding Tokens and Embeddings with LeBron James' career.\n",
    "\n",
    "### Tokens\n",
    "\n",
    "**Definition:**\n",
    "- Tokens are the smallest units of text that the model processes. They can be characters, words, or subwords, depending on the language model's tokenizer.\n",
    "\n",
    "#### Key Points:\n",
    "\n",
    "- **Role in Text Processing:** Tokens are the primary medium through which text is broken down and processed by language models. When you input text into an OpenAI model, the text is first split into tokens.\n",
    "- **Context Window:** Each model has a fixed limit on the number of tokens it can handle in a single request, known as the context window.\n",
    "- **Cost:** Usage of the API is typically measured in tokens. You are charged based on the number of tokens processed in both your input and the generated output.\n",
    "\n",
    "### Embeddings\n",
    "\n",
    "**Definition:**\n",
    "- Embeddings are numerical representations of tokens or words in a continuous vector space. These vectors capture semantic meanings and relationships between words in a high-dimensional space.\n",
    "\n",
    "#### Key Points:\n",
    "\n",
    "- **Role in Understanding Text:** While tokens are the basic units the model reads, embeddings are the way the model understands and represents these tokens internally. Each token is converted into an embedding, which is a fixed-size vector that the model uses for computations.\n",
    "- **Semantic Information:** Embeddings capture the semantic meaning of the words or tokens. For example, the words \"king\" and \"queen\" will have embeddings that are close together in the vector space, indicating their related meanings.\n",
    "- **Use Cases:** Embeddings are used in various downstream tasks like clustering, similarity searches, and as features in other machine learning models. OpenAI provides APIs specifically for generating embeddings, like the `text-embedding-ada-002` model.\n",
    "- **Dimensionality:** The embeddings themselves are fixed-dimensional vectors irrespective of the original token length. For example, a word embedding might be a 300-dimensional vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"LeBron James\"\n",
    "text2 = \"LeBron James is a professional basketball player.\"\n",
    "lebron_summary = \"\"\"\n",
    "LeBron James, born on December 30, 1984, in Akron, Ohio, is an American professional basketball player. \n",
    "He is often considered one of the greatest basketball players in the history of the NBA. James was drafted as \n",
    "the first overall pick by the Cleveland Cavaliers in the 2003 NBA draft. Over his career, he has played for \n",
    "the Miami Heat, Los Angeles Lakers, and is most known for leading his teams to multiple NBA championships. \n",
    "LeBron has won four NBA MVP Awards, two Olympic gold medals, and has numerous records and accolades.\n",
    "\"\"\"\n",
    "\n",
    "lebron1_embeddings = display_tokens_and_embeddings(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lebron2_embeddings = display_tokens_and_embeddings(text2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lebron_summary_embeddings = display_tokens_and_embeddings(lebron_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Michael Jordan Examples\n",
    "\n",
    "Understanding Tokens and Embeddings with Michael Jordan's career."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mj_text1 = \"Michael Jordan\"\n",
    "mj_text2 = \"Michael Jordan is a retired professional basketball player.\"\n",
    "mj_summary = \"\"\"\n",
    "Michael Jordan, born on February 17, 1963, in Brooklyn, New York, is widely regarded as the greatest basketball player of all time. \n",
    "He played 15 seasons in the NBA, winning six championships with the Chicago Bulls. Jordan is a five-time NBA MVP and \n",
    "a 14-time NBA All-Star. He excelled in both offense and defense, becoming a global cultural icon and a symbol of excellence \n",
    "and determination in the sport.\n",
    "\"\"\"\n",
    "\n",
    "mj1_embeddings = display_tokens_and_embeddings(mj_text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mj2_embeddings = display_tokens_and_embeddings(mj_text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mj_summary_embeddings = display_tokens_and_embeddings(mj_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Distance Metrics\n",
    "\n",
    "Distance metrics help us quantify how similar or different two embeddings are. Here are a few common ones:\n",
    "\n",
    "### Cosine Similarity\n",
    "Cosine similarity measures the cosine of the angle between two vectors. It ranges from -1 (completely dissimilar) to 1 (exactly similar).\n",
    "\n",
    "### Euclidean Distance\n",
    "Euclidean distance calculates the straight-line (geometric) distance between two points in a multidimensional space. Smaller values indicate more similarity.\n",
    "\n",
    "### Manhattan Distance\n",
    "Manhattan distance sums the absolute differences of the corresponding components of the vectors. It is robust to outliers.\n",
    "\n",
    "### Dot Product\n",
    "The dot product measures the similarity between two vectors as a single number. Higher values indicate more similarity.\n",
    "\n",
    "For this notebook, we will focus on Cosine Similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity\n",
    "\n",
    "\n",
    "Cosine similarity is a measure used to determine how similar two vectors are, irrespective of their magnitude.\n",
    "It calculates the cosine of the angle between two vectors in an inner product space. The cosine similarity metric ranges from -1 to 1:\n",
    "- **1** indicates that the vectors are identical in their direction.\n",
    "- **0** indicates that the vectors are orthogonal, meaning they are totally dissimilar.\n",
    "- **-1** indicates that the vectors are diametrically opposed.\n",
    "\n",
    "The formula for cosine similarity for two vectors is described on [wikipedia.org/wiki/Cosine_similarity](https://en.wikipedia.org/wiki/Cosine_similarity).\n",
    "Cosine similarity provides a way to measure the similarity between high-dimensional vectors, making it widely used in text analysis and embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(A, B):\n",
    "    return np.dot(A, B) / (np.linalg.norm(A) * np.linalg.norm(B))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare all LeBron embeddings to each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lebron_examples = [lebron1_embeddings, lebron2_embeddings, lebron_summary_embeddings]\n",
    "print(\"### Cosine Similarity Among LeBron James' Embeddings\")\n",
    "for i in range(len(lebron_examples)):\n",
    "    for j in range(i + 1, len(lebron_examples)):\n",
    "        similarity = cosine_similarity(lebron_examples[i], lebron_examples[j])\n",
    "        print(f\"Similarity between LeBron example {i+1} and LeBron example {j+1}: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare all Michael Jordan embeddings to each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mj_examples = [mj1_embeddings, mj2_embeddings, mj_summary_embeddings]\n",
    "print(\"### Cosine Similarity Among Michael Jordan's Embeddings\")\n",
    "for i in range(len(mj_examples)):\n",
    "    for j in range(i + 1, len(mj_examples)):\n",
    "        similarity = cosine_similarity(mj_examples[i], mj_examples[j])\n",
    "        print(f\"Similarity between MJ example {i+1} and MJ example {j+1}: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregated Embedding Comparison\n",
    "\n",
    "Aggregate embeddings for an overall representation of each player."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lebron_aggregate = np.mean(np.array(lebron_examples), axis=0)\n",
    "print(f\"length = {len(lebron_aggregate)}, head = [{lebron_aggregate[0:5]}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mj_aggregate = np.mean(np.array(mj_examples), axis=0)\n",
    "print(f\"length = {len(mj_aggregate)}, head = [{mj_aggregate[0:5]}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the aggregated embeddings of LeBron and Michael\n",
    "overall_similarity = cosine_similarity(lebron_aggregate, mj_aggregate)\n",
    "print(f\"Similarity between aggregated LeBron and aggregated Michael embeddings: {overall_similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap up\n",
    "\n",
    "Now that you have copmpleted the lab, take a moment to summarize your findings in Overleaf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "soxm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
