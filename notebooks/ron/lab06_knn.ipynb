{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 06 - Unsupervised Clustering with KNN and the Iris Dataset\n",
    "\n",
    "Start by copying this lab notebook into your notebook folder, and run it step by step from there.\n",
    "\n",
    "In this notebook, we will explore the concept of unsupervised clustering using the K-Nearest Neighbors (KNN) algorithm, using the famous Iris dataset.\n",
    "\n",
    "## Table of Contents\n",
    "1. Introduction to Machine Learning\n",
    "    * Supervised vs Unsupervised Learning\n",
    "2. Clustering, KNN and Distance Metrics\n",
    "3. Working with the Iris Dataset\n",
    "4. Varying `k` and Number of Classes\n",
    "5. The Curse of Dimensionality\n",
    "6. Summary\n",
    "7. References\n",
    "\n",
    "## Imports and Data Loading\n",
    "\n",
    "First, let's import the necessary libraries and load the Iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "df['target'] = iris.target\n",
    "df['target_name'] = df['target'].apply(lambda x: iris.target_names[x])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Introduction to Machine Learning\n",
    "\n",
    "Machine Learning (ML) is a field of artificial intelligence that uses statistical techniques to give computer systems the ability to \"learn\" from data, without being explicitly programmed.\n",
    "\n",
    "### Supervised vs. Unsupervised Learning\n",
    "- **Supervised Learning**: The algorithm is trained on a labeled dataset, which means that each training example is paired with an output label. The goal is to map the input data to the output labels.\n",
    "- **Unsupervised Learning**: The algorithm is given data without explicit instructions on what to do with it. The goal is to explore the data and find some intrinsic patterns within.\n",
    "\n",
    "## 2. Clustering, KNN, and Distance Metrics\n",
    "\n",
    "### Clustering\n",
    "Clustering is an unsupervised learning technique that involves grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar to each other than to those in other groups (clusters).\n",
    "\n",
    "### K-Nearest Neighbors (KNN)\n",
    "Although KNN is traditionally a supervised learning algorithm, it can be adapted for unsupervised learning tasks such as clustering by determining the natural grouping in the data.\n",
    "The core idea is to assign cluster memberships based on the distance to the `k` nearest neighbors.\n",
    "\n",
    "### Distance Metrics\n",
    "In KNN, the distance metric used influences the accuracy of the grouping. Common metrics include:\n",
    "- Euclidean Distance\n",
    "- Manhattan Distance\n",
    "- Minkowski Distance\n",
    "- Hamming Distance\n",
    "\n",
    "For the Iris dataset, Euclidean Distance is generally found to be effective.\n",
    "\n",
    "## 3. Working with the Iris Dataset\n",
    "\n",
    "### Visualizing Iris Data\n",
    "Let's create a pairplot to visualize the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairplot to visualize the dataset\n",
    "fig = px.scatter_matrix(df, dimensions=iris.feature_names, color='target_name')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Applying KNN Clustering\n",
    "Now, let's use the K-Nearest Neighbors algorithm to perform clustering on the Iris dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the KNN model\n",
    "k = 3\n",
    "knn = NearestNeighbors(n_neighbors=k)\n",
    "\n",
    "# Fitting the model\n",
    "knn.fit(df[iris.feature_names])\n",
    "distances, indices = knn.kneighbors(df[iris.feature_names])\n",
    "\n",
    "def assign_clusters(indices, targets):\n",
    "    \"\"\"\n",
    "    Assigns cluster labels based on the most common label among the nearest neighbors.\n",
    "    This function assumes neighboring targets are already labeled as is the case with the iris dataset.\n",
    "    If your other nearest points are not labelled you will need to use another method to assign cluster ids, such as kmeans.\n",
    "\n",
    "    Parameters:\n",
    "    indices (ndarray): A 2D array where each row contains the indices of the k nearest neighbors for a given data point.\n",
    "    targets (Series or ndarray): An array or Series containing the true labels of the data points.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of cluster labels assigned to each data point based on the most common label among its k nearest neighbors.\n",
    "    \"\"\"\n",
    "    clusters = []  # Initialize an empty list to store cluster labels\n",
    "    for row in indices:\n",
    "        neighbors_labels = targets[row]  # Get the labels of the nearest neighbors\n",
    "        most_common = np.bincount(neighbors_labels).argmax()  # Find the most common label\n",
    "        clusters.append(most_common)  # Append the most common label to the clusters list\n",
    "    return clusters  # Return the list of cluster labels\n",
    "\n",
    "df['cluster'] = assign_clusters(indices, df['target'])\n",
    "\n",
    "# Plotting the clusters (using first two features)\n",
    "fig = px.scatter(df, x='sepal length (cm)', y='sepal width (cm)', color='cluster', \n",
    "                 title='KNN Clustering with k=3 (Sepal Length vs Sepal Width)')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Varying `k` and Number of Classes\n",
    "\n",
    "Let's explore how varying the `k` value affects clustering.\n",
    "\n",
    "### Experiment with Different `k` Values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = [1, 3, 5]\n",
    "figs = []\n",
    "\n",
    "for k in k_values:\n",
    "    knn = NearestNeighbors(n_neighbors=k)\n",
    "    knn.fit(df[iris.feature_names])\n",
    "    distances, indices = knn.kneighbors(df[iris.feature_names])\n",
    "    df['cluster'] = assign_clusters(indices, df['target'])\n",
    "  \n",
    "    fig = px.scatter(df, x='sepal length (cm)', y='sepal width (cm)', color='cluster', \n",
    "                     title=f\"Clustering with k={k} (Sepal Length vs Sepal Width)\")\n",
    "    figs.append(fig)\n",
    "\n",
    "for fig in figs:\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment with differnt numbers of clusters.\n",
    "\n",
    "Even though the Iris dataset has 3 classes, we can use various cluster counts (e.g., 2, 3, 4, 5) to see how the clustering results change.\n",
    "\n",
    "First, let's define a function that assigns clusters based on forced number of clusters using KMeans, since KNN doesn't natively support clustering configuration but rather classification. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_clusters(n_clusters, data, feature_x, feature_y, title):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(data)\n",
    "    df['cluster'] = kmeans.labels_\n",
    "    \n",
    "    fig = px.scatter(df, x=feature_x, y=feature_y, color='cluster', \n",
    "                     title=title, labels={feature_x: feature_x, feature_y: feature_y})\n",
    "    fig.show()\n",
    "\n",
    "# Visualize clusters for different n_clusters\n",
    "n_clusters_values = [2, 3, 4, 5]\n",
    "for n_clusters in n_clusters_values:\n",
    "    visualize_clusters(n_clusters, df[iris.feature_names], 'sepal length (cm)', 'sepal width (cm)', \n",
    "                       f'Clustering with n_clusters={n_clusters} (Sepal Length vs Sepal Width)')\n",
    "\n",
    "    # visualize_clusters(n_clusters, df[iris.feature_names], 'petal length (cm)', 'petal width (cm)', \n",
    "    #                    f'Clustering with n_clusters={n_clusters} (Petal Length vs Petal Width)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. The Curse of Dimensionality\n",
    "\n",
    "The \"curse of dimensionality\" refers to the various phenomena that arise when analyzing and organizing data in high-dimensional spaces. Often, the intuition we have for low-dimensional spaces does not apply.\n",
    "\n",
    "### Implications for KNN\n",
    "- As dimensionality increases, the distance between any two points tends to become similar, making it difficult to distinguish between different points or clusters.\n",
    "- This can degrade the performance of KNN because the nearest neighbor of a given point may be far less similar in high-dimensional space than in low-dimensional space.\n",
    "\n",
    "### Dimensionality Reduction\n",
    "- Techniques like PCA (Principal Component Analysis) or t-SNE (t-distributed Stochastic Neighbor Embedding) can be used to reduce the dimensionality of data before applying KNN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reducing dimensions to 2 using PCA\n",
    "pca = PCA(n_components=2)\n",
    "reduced_data = pca.fit_transform(df[iris.feature_names])\n",
    "\n",
    "# Applying KNN on reduced data\n",
    "knn = NearestNeighbors(n_neighbors=3)\n",
    "knn.fit(reduced_data)\n",
    "distances, indices = knn.kneighbors(reduced_data)\n",
    "df['cluster_pca'] = assign_clusters(indices, df['target'])\n",
    "\n",
    "# Plotting the reduced clusters\n",
    "fig = px.scatter(df, x=reduced_data[:, 0], y=reduced_data[:, 1], color='cluster_pca', \n",
    "                 title=\"Clustering on Reduced Data (PCA Components)\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Summary\n",
    "\n",
    "- **Dimensionality Reduction Matters**: Reducing dimensions can improve the performance of KNN in high-dimensional data.\n",
    "- **Distance Metrics Matter**: The choice of distance metric influences the accuracy of clustering.\n",
    "- **`k` Matters**: The value of `k` impacts the composition and the number of clusters significantly.\n",
    "- **Number of Classes Matter**: The natural number of classes in data can influence the results of clustering.\n",
    "\n",
    "By understanding these key aspects, we can apply clustering methods more effectively to any dataset.\n",
    "\n",
    "## 7. References\n",
    "- [Scikit-learn Documentation](https://scikit-learn.org/stable/documentation.html)\n",
    "- [Plotly Express Documentation](https://plotly.com/python/plotly-express/)\n",
    "- [Curse of Dimensionality Article on Wikipedia](https://en.wikipedia.org/wiki/Curse_of_dimensionality)\n",
    "\n",
    "## Wrap up\n",
    "\n",
    "Update your Overleaf with lessons learned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "soxm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
